
数据规整化

一、数据库风格的DF合并
1.merge和join通过一个或多个键将行连接起来。
pd.merge(df1,df2,on='key')#如果没有指定，merge就会将重叠列的列名当作主键。最好显式指定一下

2.左右不同时
pd.merge(df1,df2,left_on='lkey',right_on='rkey')
pd.merge(df1,df2,on=['key1','key2'],how='outer')#利用列表来表示
默认情况下，是内部连接，结果是交集。

3.设置连接方式，left，right，outer
pd.merge(df1,df2,on='key',how='left')

4.对重复列名的表示,suffixes
pd.merge(left,right,on='key',suffixes=('_left','_right')

5.连接键位于索引中，left_index=True,right_index=True,来选择
pd.merge(left,right,left_on='key',right_index=True,how='outer')
#把left里的key列和right里的索引合并,合并方式为外联结。

6.如果是层次化索引，需要用列表说明
pd.merge(left,right,left_on=['key1','key2'],right_index=True)

7.如果同时都是索引来联结，则两个都标上。（这时候似乎不用on了）
pd.merge(left,right,left_index=True,right_index=True,how='outer')

8.join函数的方法联结
left.join(right,on='key')

二、轴向连接concat
1.numpy的concatenate
np.concatenate([arr,arr],axis=1)

2.没有重叠索引的Series
pd.concat([s1,s2,s3])
#默认情况下，concat是在axis=0的轴上工作，并最终产生一个新的Series。
#如果把axis设置为1，则生成一个DF
pd.concat([s1,s2,s3],axis=1)

3.通过join_axes来指定要在其他轴上的索引
pd.concat([s1,s4],axis=1,join_axes=[['a','b','c','d']]

4.给每个合并的表分别加入名称以区分
result=pd.concat([s1,s1,s3],keys=['one','two','three'])
result.unstack()

5.对PD的合并
df1=DataFrame(np.arange(6).reshape(3,2),index=['a','b','c'],columns=['one','two'])#1.本身有index；2.columns本身后面有s
df2=DataFrame(np.arange(4).reshape(2,2),index=['a','c'],columns=['three','four'])
pd.concat([df1,df2],axis=1,keys=['level1','level2'],names=['upper','lower'])
#横向合并(添加列)，合并时用index来，并且指定keys来分别对df用level1和level2来标注。upper和lower分别对应level1和level2的层次索引。

6.join='inner'内联结，会去除NaN值

7.与当前工作无关的行索引。ignore_index=True
pd.concat([df1,df2],ignore_index=True)
#合并时会直接照着列排下去

三、合并重叠数据、轴向旋转

1.利用NumPy的where函数
np.where(pd.insnull(a),b,a)
#在a处为Nan的地方放入b，其他地方是a。相当于以a为基础来合并b值。

2.Series里combine_first
b[:-2].combine_first(a[2:])
#以b倒数第二项之前的数据为基础，合并a第三项及之后的数据

3.DataFrame里的combine_first
df1.combine_first(df2)
#以df1为基础，空值合并为df2相关数值。

4.stack将数据列旋转为行；unstack将数据行旋转为列。
对于二维PD，列转为行，会得到一个Series；而对于双重索引的Series，由unstack可以把Series变为二维PD。
resutlt=data.stake()
#默认情况下是对最内层进行旋转，指定层次可以由外层开始旋转。层次的编号是由外向内的。
#例如，state是最外层，number是内层，想要把最外层的state旋转出来
result.unstack(0);result.unstack('state')#指定名比较好，不会乱。

5.如果不是两个Series都有的话，unstack会引入缺失值。而stack时默认会把缺失值删去。
#如果想保留缺失值，则需要设置 dropna=False
data.unstack().stack(dropna=False)

6.对DataFrame进行unstack时，作为旋转轴的级别将会成为结果中的最低级别（最内层）（这里有疑问，因为最内层不是0么？那怎么之前unstack（0）时是对最外层呢？）
df.unstack('state').stack('side')
#state由行专向列的最内层，side由列转向行的最内层。

7.对MySQL里的长格式转为宽格式-pivot
时间序列数据通常以长格式（固定架构：列名和数据关系）储存在数据表中。
固定架构的好处：随着表中数据的添加或减少，item列中的值的种类能够增加或减少。（没明白，其他的就不能增减了？）
缺点：长格式的数据操作起来不轻松。
DataFrame特点：不同的item值分别形成一列，date列中的时间值则用作索引。

8.pivot转MySQL为DF
#原数据表只有三列，data可做行，item做列，最后value是值。
pivoted = Idata.pivot('date','item','value')
#转换后，变为date行，item列，value值的DF

9.对于多个value值
忽略最后一个参数，则value变为列的最外层
pivoted=Idata.pivot('date','item')
其他不同的列形成最外方的索引。


四、数据转换

1.移除重复值drop_duplicates
duplicated,返回布尔值
drop_duplicates,返回一个移除了重复行的DataFrame
#注意这两个命令的拼写，一个是加d，一个是加s。
data.drop_duplicates()
#默认判断全部列，如果只是希望对某一列判断
data.drop_duplicates(['k1'])
#默认是判断和保留第一个，如果是要保留最后一个，则take_last=True
data.drop_duplicates(['k1','k2'],take_last=True)

2.函数映射map，产生新的列
data=DataFrame({'food':['bacon','pulled pork','bacon','Pastrami','corned beef','Bacon','pastrami','honey ham','nova lox'],'ounces':[4,3,12,6,7.5,8,3,5,6]})
data
#添加一个肉类食物来源的种类。这里字典都是x：y，因此，左边是肉，右边是来源值。
meat_to_animal = {'bacon':'pig','pulled pork':'pig','pastrami':'cow','corned beef':'cow','honey ham':'pig','nova lox':'salmon'}
#Series里的map可以接受一个函数或含有映射关系的字典型对象。
#处理时，先转换大小写，然后转换对应值
data['animal']=data['food'].map(str.lower).map(meat_to_animal)
#或者，直接传入一个lambda函数
data['food'].map(lambda x: meat_to_animal[x.lower()])

3.替换值replace，替换现有值
data=Series([1.,-999.,2.,-999.,-1000.,3.])
data.replace([-999,-1000],np.nan)#把-999和-1000都转换成nan值
data.replace([-999,-1000],[np.nan,0])#分别转换为不同的值
#通过字典的方式转换
data.replace({-999:np.nan,-1000:0})

4.对索引轴数据处理map,rename,inplace=True
修改原数据值，map
data=DataFrame(np.arange(12).reshape((3,4)),index=['Ohio','Colorado','New York'],columns=['one','two','three','four'])
data.index.map(str.upper)#并未修改，只是看看
data.index=data.index.map(str.upper)#修改了

5.创建数据集的转换版
data.rename(index=str.title,columns=str.upper)
#这里对索引和列重新命名了，用的是str的内部函数title和upper。
data.rename(index={'Ohio':'Indiana'},columns={'three':'peekaboo'})
#分别对index和columns内的数据进行替换

6.rename操作默认没有直接更改原index，如果要就地修改，需要inplace=True
_ = data.rename(index={'Ohio'='Indiana'},inplace=True)
#这里用'_'能够总是返回DataFrame的引用，挺神奇
#设置inplace=True,就能够就地修改原表值了。

7.对数据分组cut（可应用于通达信强度值）
 7.1#有些MySQL数据库的味道
	ages=[20,22,25,27,21,23,37,31,61,45,41,32]
	bins=[18,25,35,60,100]#设定分组标准
	cats=pd.cut(ages,bins)#对ages数值应用以bins分类标准的cut分组操作
	#检查cats在各个分组中的数目
	pd.value_counts(cats)


 7.2#默认为左开右闭，可修改开闭区间。例如修改为左闭右开，right=False

 7.3#设置分组名称
 group_names = ['Youth','YoungAdult','MiddleAged','Senior']
 pd.cut(ages,bins,label=group_names)#这里是label，而不是lable

8.qcut，利用样本分位数来分组
按照分位数来切割数据，获得大小基本相同的块
data=np.random.randn(1000)#正态分布
cats=pd.qcut(data,4)#按四分位数进行切割
pd.qcut(data,[0,0.1,0.5,0.9,1]#按照自己设置的分位数进行分组

9.过滤异常值基本是数组运算


五、其他

1.利用take来重排
df=DataFrame(np.arange(5*4).reshape(5,4))
sampler=np.random.permutation(5)#生成从（0-4）的随机整数数列
df.take(sampler)#按照列来变换数据顺序
df.take(np.random.permutation(len(df)),[:3])#获取跟表长度值一样的值，重排后只保留前3个。

2.计算哑变量dummy(哑变量是另一矩阵，类似于一种分类
df=DataFrame({'key':['b','b','a','c','a','b'],'data1':range(6)})
pd.get_dummies(df['key'])

3.字符串操作

4.正则表达式

